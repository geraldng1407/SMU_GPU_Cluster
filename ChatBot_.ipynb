{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoQ4_TlXybJ9"
      },
      "source": [
        "###Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mVkBeAG-dTPF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-iJx8HwyhLr"
      },
      "source": [
        "###Reading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RsJ7lLK1Cay_"
      },
      "outputs": [],
      "source": [
        "file = open('./data/Dataset.txt','r').read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sSNomuT7Jsvv"
      },
      "outputs": [],
      "source": [
        "raw_data = [f.split('\\t') for f in file.split('\\n')]    #separating questions and answers\n",
        "questions = [x[0] for x in raw_data]\n",
        "answers = [x[1] if len(x) > 1 else \"\" for x in raw_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiA-0GwtJwYd",
        "outputId": "dddbbb76-4209-402a-e39e-fc296d863f8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  hi, how are you doing?\n",
            "Answer:  i'm fine. how about yourself?\n"
          ]
        }
      ],
      "source": [
        "print(\"Question: \", questions[0])\n",
        "print(\"Answer: \", answers[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFuAVvFuyo64"
      },
      "source": [
        "###Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WXM48uqPI24W"
      },
      "outputs": [],
      "source": [
        "def tokenize(lang):\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "    return tensor, lang_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P57M5HwNyrtM"
      },
      "source": [
        "###PreProcessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qtQWA702Iwxx"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = ''.join(c for c in unicodedata.normalize('NFD', sentence) if unicodedata.category(c) != 'Mn')\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
        "    sentence = '<start> ' + sentence + ' <end>'\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ubN0jC6zJ0hH"
      },
      "outputs": [],
      "source": [
        "pre_questions = [preprocess_sentence(w) for w in questions] #processing all the quesstions\n",
        "pre_answers = [preprocess_sentence(w) for w in answers] #processing all the answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EsOXfre3KSDy"
      },
      "outputs": [],
      "source": [
        "data = pre_answers, pre_questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ryMnanp0KBWw"
      },
      "outputs": [],
      "source": [
        "def prepare_data(data):\n",
        "    targ_lang, inp_lang = data\n",
        "\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xJV_dqtyKYzm"
      },
      "outputs": [],
      "source": [
        "input_tensor, target_tensor, inp_lang, targ_lang = prepare_data(data)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQAe2JYuJycK",
        "outputId": "d06bcf15-929a-4fd4-b941-6b8b49a590a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24\n",
            "24\n"
          ]
        }
      ],
      "source": [
        "print(max_length_targ)\n",
        "print(max_length_inp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7uEiPyo7DyI"
      },
      "source": [
        "### Downloading the Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Fjor8_8K6__v"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def save_tokenizer(tokenizer, filename):\n",
        "    with open(filename, 'wb') as handle:\n",
        "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "save_tokenizer(inp_lang, 'input_tokenizer.pkl')\n",
        "save_tokenizer(targ_lang, 'target_tokenizer.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqptaUlQyw5m"
      },
      "source": [
        "###Splitting the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXEJ-kkDWDIH",
        "outputId": "a318fd4c-bb00-4f08-c4d1-5f3f9f0db71d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the data into 90% train, 10% validation\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(\n",
        "    input_tensor, target_tensor, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3nQQ917yzKF"
      },
      "source": [
        "###Defining the PipeLine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUeolMd9SKHY",
        "outputId": "2578c4de-3cd7-4a59-8b8f-1ee399c421f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorShape([64, 24]), TensorShape([64, 24]))"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 400\n",
        "units = 1500\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Nk-cafrc3WM8"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                         return_sequences=True,\n",
        "                                         return_state=True,\n",
        "                                         recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "      x = self.embedding(x)\n",
        "      output, state_h, state_c = self.lstm(x, initial_state=hidden)  # Use LSTM with state_h and state_c\n",
        "      state = [state_h, state_c]\n",
        "      return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return [tf.zeros((self.batch_sz, self.enc_units)),\n",
        "                tf.zeros((self.batch_sz, self.enc_units))]\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "adO6-QxYKlwx"
      },
      "outputs": [],
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ucM7JAxI4vM3"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                         return_sequences=True,\n",
        "                                         return_state=True,\n",
        "                                         recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        self.attention = Attention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden[0], enc_output)\n",
        "\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Concatenate context vector and embedding\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # Passing the concatenated vector to the LSTM\n",
        "        output, state_h, state_c = self.lstm(x, initial_state=hidden)  # Use LSTM with state_h and state_c\n",
        "\n",
        "        state = [state_h, state_c]\n",
        "\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uHuaQpny3Wk"
      },
      "source": [
        "###Adjusting Learning Rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Z0Z62qj-ZLru"
      },
      "outputs": [],
      "source": [
        "initial_learning_rate = 0.001 #adaptive learning rate\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True\n",
        ")\n",
        "\n",
        "# Define the optimizer with adaptive learning rate\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "# Define your loss function\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none'\n",
        ")\n",
        "\n",
        "def loss_function(real, pred):  #defining loss function\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXFR4CAxy9Od"
      },
      "source": [
        "###Defining Train Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ftqY-Rj_XNlW"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "        # Initialize LSTM's initial state\n",
        "        dec_hidden = [enc_hidden[0][:, :units], enc_hidden[1][:, :units]]\n",
        "\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # Passing enc_output to the decoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # Using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lo-wQ2NPXOKm"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def validation_step(inp, targ, enc_hidden):\n",
        "    val_loss = 0\n",
        "\n",
        "    val_samples = 0\n",
        "\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    # Initialize LSTM's initial state\n",
        "    dec_hidden = [enc_hidden[0][:, :units], enc_hidden[1][:, :units]]\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    for t in range(1, targ.shape[1]):\n",
        "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "        loss = loss_function(targ[:, t], predictions)\n",
        "        val_loss += loss\n",
        "        val_samples += 1\n",
        "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    val_loss /= val_samples\n",
        "\n",
        "    return val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWKFyJ9ey__t"
      },
      "source": [
        "###Training the Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fEBjc3KTdgE-",
        "outputId": "73715586-df7f-4408-fcb4-a43dcf751531"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  1 Loss:2.0346 Val Loss:1.8957\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "EPOCHS = 60\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    # Training loop\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "    num_samples = 0\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "      enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "      dec_hidden = enc_hidden\n",
        "      dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "      for t in range(1, targ.shape[1]):\n",
        "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "        predicted_id = tf.argmax(predictions, axis=-1)\n",
        "        num_samples += 1\n",
        "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    validation_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n",
        "    validation_dataset = validation_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "    val_loss = 0\n",
        "    val_samples = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(validation_dataset):\n",
        "      enc_hidden = encoder.initialize_hidden_state()  # Initialize hidden state for each batch\n",
        "      val_batch_loss = validation_step(inp, targ, enc_hidden)\n",
        "      val_loss += val_batch_loss\n",
        "      val_samples += 1\n",
        "\n",
        "    val_loss /= val_samples\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        train_losses.append(total_loss / steps_per_epoch)\n",
        "        val_losses.append(val_loss)\n",
        "        print('Epoch:{:3d} Loss:{:.4f} Val Loss:{:.4f}'.format(\n",
        "            epoch, total_loss / steps_per_epoch,  val_loss))\n",
        "\n",
        "# Plotting the accuracy and loss graphs\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot training and validation losses\n",
        "plt.plot(range(1, EPOCHS + 1), train_losses, label='Train')\n",
        "plt.plot(range(1, EPOCHS + 1), val_losses, label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#training took 3 hours to complete with colab GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c2YfFId3U5Y",
        "outputId": "9898c4b4-4df7-428d-a985-836d58553552"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) args_1 with unsupported characters which will be renamed to args_1_1 in the SavedModel.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Function `_wrapped_model` contains input name(s) args_1 with unsupported characters which will be renamed to args_1_1 in the SavedModel.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n",
            "WARNING:absl:<__main__.Attention object at 0x7ff671ded3f0> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ],
      "source": [
        "encoder.save(\"encoder_final\")\n",
        "decoder.save(\"decoder_final\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12f5dE5E3XZ-",
        "outputId": "1990d3cf-3808-46d9-d857-b5977b77f83e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: encoder_final/ (stored 0%)\n",
            "  adding: encoder_final/variables/ (stored 0%)\n",
            "  adding: encoder_final/variables/variables.index (deflated 39%)\n",
            "  adding: encoder_final/variables/variables.data-00000-of-00001 (deflated 7%)\n",
            "  adding: encoder_final/keras_metadata.pb (deflated 80%)\n",
            "  adding: encoder_final/fingerprint.pb (stored 0%)\n",
            "  adding: encoder_final/saved_model.pb (deflated 91%)\n",
            "  adding: encoder_final/assets/ (stored 0%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r \"encoder_final.zip\" \"encoder_final\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14lbjgIC3acq",
        "outputId": "db133072-6631-413a-d259-251ad41d4eda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: decoder_final/ (stored 0%)\n",
            "  adding: decoder_final/variables/ (stored 0%)\n",
            "  adding: decoder_final/variables/variables.index (deflated 52%)\n",
            "  adding: decoder_final/variables/variables.data-00000-of-00001 (deflated 7%)\n",
            "  adding: decoder_final/keras_metadata.pb (deflated 87%)\n",
            "  adding: decoder_final/fingerprint.pb (stored 0%)\n",
            "  adding: decoder_final/saved_model.pb (deflated 90%)\n",
            "  adding: decoder_final/assets/ (stored 0%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r \"decoder_final.zip\" \"decoder_final\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gFtSqTElXltk"
      },
      "outputs": [],
      "source": [
        "def remove_tags(sentence):\n",
        "    return sentence.split(\"<start>\")[-1].split(\"<end>\")[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "w3qbBcT6IPkf"
      },
      "outputs": [],
      "source": [
        "def evaluate(sentence):\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return remove_tags(result), remove_tags(sentence)\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return remove_tags(result), remove_tags(sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing some random questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG40LpHdONCG",
        "outputId": "bc298204-fd4f-4d08-9b2e-cc0222eedcd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  good luck with school \n",
            "Predicted answer: thank you very much . \n"
          ]
        }
      ],
      "source": [
        "def test(question):\n",
        "    answer, question = evaluate(question)\n",
        "    print('Question:', question)\n",
        "    print('Predicted answer:', answer)\n",
        "\n",
        "test(\"good luck with school\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99zBE0TPZ8WG",
        "outputId": "dd68980d-b4e5-4fb1-8a8d-87f303e5c5e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  hello \n",
            "Predicted answer: greetings ! \n",
            "Question:  how are you doing ? \n",
            "Predicted answer: fine , and you ? \n",
            "Question:  what is your age ? \n",
            "Predicted answer: i am still young by your standards . \n",
            "Question:  do you have a tv ? \n",
            "Predicted answer: yes , i do . \n",
            "Question:  do you like rain ? \n",
            "Predicted answer: yes , i love traveling and exploring new places . \n"
          ]
        }
      ],
      "source": [
        "test(\"Hello\")\n",
        "test(\"How are you doing?\")\n",
        "test(\"What is your age?\")\n",
        "test(\"Do you have a tv?\")\n",
        "test(\"Do you like rain?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvz7kNlj25-K",
        "outputId": "4e32399c-d419-40c7-a278-e350139045c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  i am afraid \n",
            "Predicted answer: why ? do i frighten you ? try not to be too scared . what are you afraid of ? \n"
          ]
        }
      ],
      "source": [
        "test(\"I am afraid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJoKOlrM28e9",
        "outputId": "f4a5539c-0924-4be7-fc7c-d12c3d10a299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  i am feeling sick \n",
            "Predicted answer: oh , really ? \n"
          ]
        }
      ],
      "source": [
        "test(\"I am feeling sick\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCDxO6fiNyjr",
        "outputId": "3c971737-1215-4dba-c5d0-e6648372b41d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  sorry \n",
            "Predicted answer: yeah , so do i . \n"
          ]
        }
      ],
      "source": [
        "test(\"Sorry\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11vTxcJRN0AS",
        "outputId": "453c65bd-87c2-4325-9acf-667197149c2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  hi , how are you doing ? \n",
            "Predicted answer: i m fine . how about yourself ? \n"
          ]
        }
      ],
      "source": [
        "test(\"hi, how are you doing?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGgZlEWfN5sG",
        "outputId": "77507157-5750-48eb-da97-0969e0489d3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  i m pretty good . thanks for asking . \n",
            "Predicted answer: no problem . so how have you been ? \n"
          ]
        }
      ],
      "source": [
        "test(\"i'm pretty good. thanks for asking.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61qs3yLVN8Ko",
        "outputId": "7cd07063-8f05-4f72-8f38-36118072cf43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  i ve been great . what about you ? \n",
            "Predicted answer: i ve been good . i m in school right now . \n"
          ]
        }
      ],
      "source": [
        "test(\"i've been great. what about you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTylB062N_qd",
        "outputId": "687e05c3-e014-4ef9-f0c3-ce1f642470f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  what school do you go to ? \n",
            "Predicted answer: i go to pcc . \n"
          ]
        }
      ],
      "source": [
        "test(\"what school do you go to?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ3xo_rNOEwB",
        "outputId": "5321db08-bfce-4614-8398-80a4908cc254"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  i don t know \n",
            "Predicted answer: i like the ones i can sing along with . \n"
          ]
        }
      ],
      "source": [
        "test(\"I don't know\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvaXsRBMOGk9",
        "outputId": "810c7d02-4152-4603-9900-6b3cf4504e2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  nice to meet you \n",
            "Predicted answer: thank you . \n"
          ]
        }
      ],
      "source": [
        "test(\"nice to meet you\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOrbnn6zOJvj",
        "outputId": "8ce3c076-7169-4fc8-99e8-0d046f0bf41f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  what are your hobbies ? \n",
            "Predicted answer: i enjoy reading books and playing the guitar . \n"
          ]
        }
      ],
      "source": [
        "test(\"What are your hobbies?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le5qUCLbOMAF",
        "outputId": "eaf250ac-7a94-44df-f01f-f38fd8ccba0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  you are rude \n",
            "Predicted answer: yep . i always behave in socially unacceptable ways . \n"
          ]
        }
      ],
      "source": [
        "test(\"You are rude\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKqioan4ONn6",
        "outputId": "bf2f7666-1e59-4d20-d7e0-3f52b7642441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  i love you \n",
            "Predicted answer: i love you , too . \n"
          ]
        }
      ],
      "source": [
        "test(\"I love you\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB1yEq__OTEw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
